[{"authors":null,"categories":null,"content":" I graduated Summa Cum Laude from Vietnam National University Ho Chi Minh City - University of Science.\nPreviously at VinAI, I was a Research Resident in ML robustness and interpretability and an Engineering Resident in automotive perception and user interfaces.\nUnder the supervision of amazing mentors, my past experiences had spanned across multiple disciplines including implicit sensing, time-varying signal processing, knowledge discovery with graph neural networks, decentralized computing, and blockchain.\nLooking forward, I want to develop trustworthy algorithms and devices that assist healthcare providers in enhancing the quality of care.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I graduated Summa Cum Laude from Vietnam National University Ho Chi Minh City - University of Science.\nPreviously at VinAI, I was a Research Resident in ML robustness and interpretability and an Engineering Resident in automotive perception and user interfaces.","tags":null,"title":"Tuan-Duy H. Nguyen","type":"authors"},{"authors":null,"categories":null,"content":"Pre-trained VinAI Translate models vinai/vinai-translate-vi2en and vinai/vinai-translate-en2vi are state-of-the-art text translation models for Vietnamese-to-English and English-to-Vietnamese, respectively. Our demonstration system VinAI Translate employing these pre-trained models is available at: https://vinai-translate.vinai.io.\n","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"252ec1d7c59d7ac1bdda01b75c375161","permalink":"https://duynht.github.io/publication/vi_translation/","publishdate":"2022-07-20T00:00:00Z","relpermalink":"/publication/vi_translation/","section":"publication","summary":"We present VinAI Translateâ€”a system that integrates state-of-the-art deep  learning technologies in speech and natural language processing to translate speech and text between Vietnamese and English. Experimental results show that our system obtains a state-of-the-art performance for each translation direction, and outperforms Google Translate in both automatic and human evaluations.","tags":null,"title":"A Vietnamese-English Neural Machine Translation System (INTERSPEECH 2022 Show \u0026 Tell)","type":"publication"},{"authors":null,"categories":null,"content":"We introduce a model-agnostic recourse that minimizes the posterior probability odds ratio along its min-max robust counterpart with the goal of hedging against future changes in the machine learning model parameters. Robust Bayesian Recourse explicitly takes into account possible perturbations of the data in a Gaussian mixture ambiguity set prescribed using the optimal transport (Wasserstein) distance.\n","date":1659312e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312e3,"objectID":"2d59a8c8f6e7ca323b863dc90ff6a7d4","permalink":"https://duynht.github.io/publication/robust_bayesian_recourse/","publishdate":"2022-07-20T00:00:00Z","relpermalink":"/publication/robust_bayesian_recourse/","section":"publication","summary":"Algorithmic recourse aims to recommend an informative feedback to overturn an unfavorable machine learning decision. We introduce in this paper the Bayesian recourse, a model-agnostic recourse that minimizes the posterior probability odds ratio. Further, we present its min-max robust counterpart with the goal of hedging against future changes in the machine learning model parameters. The robust counterpart explicitly takes into account possible perturbations of the data in a Gaussian mixture ambiguity set prescribed using the optimal transport (Wasserstein) distance. We show that the resulting worst-case objective function can be decomposed into solving a series of two-dimensional optimization subproblems, and the min-max recourse finding problem is thus amenable to a gradient descent algorithm. Contrary to existing methods for generating robust recourses, the robust Bayesian recourse does not require a linear approximation step. The numerical experiment demonstrates the effectiveness of our proposed robust Bayesian recourse facing model shifts. Our code is available at https://github.com/VinAIResearch/robust-bayesian-recourse.","tags":null,"title":"Robust Bayesian Recourse (UAI 2022)","type":"publication"},{"authors":null,"categories":null,"content":"We develop QA-CarManual as a lightweight, real-time and interactive system that integrates state-of-the-art technologies in language and speech processing to (i) understand and interact with users via speech commands and (ii) automatically query the knowledge base and return answers in both forms of text and speech as well as visualization.\n","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"0fdd71a783b8515195a90b20dd71ed18","permalink":"https://duynht.github.io/publication/vi_carqa/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/vi_carqa/","section":"publication","summary":"This paper presents a novel Vietnamese speech-based question answering system QA-CarManual that enables users to ask car-manual-related questions (e.g. how to properly operate devices and/or utilities within a car). Given a car manual written in Vietnamese as the main knowledge base, we develop QA-CarManual as a lightweight, real-time and interactive system that integrates state-of-the-art technologies in language and speech processing to (i) understand and interact with users via speech commands and (ii) automatically query the knowledge base and return answers in both forms of text and speech as well as visualization. To our best knowledge, QA-CarManual is the first Vietnamese question answering system that interacts with users via speech inputs and outputs. We perform a human evaluation to assess the quality of our QA-CarManual system and obtain promising results.","tags":null,"title":"Vietnamese Speech-based Question Answering over Car Manuals (DCAI@NeurIPS2021 and IUI 2022)","type":"publication"},{"authors":null,"categories":null,"content":"This paper presents the methods that have participated in the SHREC 2021 contest on retrieval and classification of protein surfaces on the basis of their geometry and physicochemical properties.\n","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"284798ca2cb347f8b9eaba7f4a6871b0","permalink":"https://duynht.github.io/publication/protein_classes/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/protein_classes/","section":"publication","summary":"This paper presents the methods that have participated in the SHREC 2021 contest on retrieval and classification of protein surfaces on the basis of their geometry and physicochemical properties. The goal of the contest is to assess the capability of different computational approaches to identify different conformations of the same protein, or the presence of common sub-parts, starting from a set of molecular surfaces. We addressed two problems: defining the similarity solely based on the surface geometry or with the inclusion of physicochemical information, such as electrostatic potential, amino acid hydrophobicity, and the presence of hydrogen bond donors and acceptors. Retrieval and classification performances, with respect to the single protein or the existence of common sub-sequences, are analysed according to a number of information retrieval indicators.","tags":null,"title":"SHREC 2021: Retrieval and classification of protein surfaces equipped with physical and chemical properties (CAG Vol. 99)","type":"publication"},{"authors":null,"categories":null,"content":"This paper assesses the ability of five methods to retrieve similar protein surfaces, using either their shape only (3D meshes), or their shape and the electrostatic potential at their surface, an important surface property.\n","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"8a7bc751fefcbe2ea543746702a43ba5","permalink":"https://duynht.github.io/publication/protein_domains/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/publication/protein_domains/","section":"publication","summary":"Proteins are essential to nearly all cellular mechanism, and often interact through their surface with other cell molecules, such as proteins and ligands. The evolution generates plenty of different proteins, with unique abilities, but also proteins with related functions hence surface, which is therefore of primary importance for their activity. In the present work, we assess the ability of five methods to retrieve similar protein surfaces, using either their shape only (3D meshes), or their shape and the electrostatic potential at their surface, an important surface property. Five different groups participated in this challenge using the shape only, and one group extended its pre-existing algorithm to handle the electrostatic potential. The results reveal both the ability of the methods to detect related proteins and their difficulties to distinguish between topologically related proteins.","tags":null,"title":"SHREC 2021: Surface-based Protein Domains Retrieval (3DOR 2021)","type":"publication"},{"authors":null,"categories":null,"content":"Five different groups participated in this contest using the shape-only dataset, and one group extended its pre-existing method to handle the electrostatic potential. Our comparative study reveals both the ability of the methods to detect related proteins and their difficulties to distinguish between highly related proteins. Our study allows also to analyze the putative influence of electrostatic information in addition to the one of protein shapes alone.\n","date":1630540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630540800,"objectID":"fd57f2abe3e4e1f1a1d2ec34e5f44ba6","permalink":"https://duynht.github.io/publication/protein_domains_journal/","publishdate":"2021-09-02T00:00:00Z","relpermalink":"/publication/protein_domains_journal/","section":"publication","summary":"Proteins are essential to nearly all cellular mechanism and the effectors of the cells activities. As such, they often interact through their surface with other proteins or other cellular ligands such as ions or organic molecules. The evolution generates plenty of different proteins, with unique abilities, but also proteins with related functions hence similar 3D surface properties (shape, physico-chemical properties, â€¦). The protein surfaces are therefore of primary importance for their activity. In the present work, we assess the ability of different methods to detect such similarities based on the geometry of the protein surfaces (described as 3D meshes), using either their shape only, or their shape and the electrostatic potential (a biologically relevant property of proteins surface). Five different groups participated in this contest using the shape-only dataset, and one group extended its pre-existing method to handle the electrostatic potential. Our comparative study reveals both the ability of the methods to detect related proteins and their difficulties to distinguish between highly related proteins. Our study allows also to analyze the putative influence of electrostatic information in addition to the one of protein shapes alone. Finally, the discussion permits to expose the results with respect to ones obtained in the previous contests for the extended method. The source codes of each presented method have been made available online.","tags":null,"title":"Surface-based protein domains retrieval methods from a SHREC2021 challenge (JMGM Vol. 111)","type":"publication"},{"authors":null,"categories":null,"content":"In this thesis, we set forth to extend this line of research for intuitive, effortless, and enjoyable computer interaction by employing a natural everyday-carry object \u0026ndash; the smartwatch.\n","date":1630108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630108800,"objectID":"24e5d6efbadc97f968f3a8748895a3ed","permalink":"https://duynht.github.io/publication/accel_watch/","publishdate":"2021-08-28T00:00:00Z","relpermalink":"/publication/accel_watch/","section":"publication","summary":"Despite computers are created to serve humans and come for their needs, interaction with computers nowadays is an obstruction to our societal life. Computers, being it your laptop or smartphone, require significant attention from us to operate. Every time you look down to text on your phone screen is a moment you disengage from reality. This is due to the lack of intuitive and implicit input capabilities for computer and such minuscule but repetitive behavior have turned people to prefer keyboards and screens over talking and interacting with the physical world. Over the past decade, there have been significant efforts in addressing this challenge via brave new interactive modalities that could weave seamlessly into our life such as smart glasses, smart rings, smart speakers, etc.\nIn this thesis, we set forth to extend this line of research for intuitive, effortless, and enjoyable computer interaction by employing a natural everyday-carry object -- the smartwatch. We look for extending the capability of the smartwatch beyond what it already is, i.e., a timepiece and mostly a health monitor, and become a mean to facilitate an effortless human-computer away from mice and keyboards. We follow through, first by enhancing the capability of the smartwatch; then we propose a technique for activity recognition using inertial motion signals from the smartwatch; and finally, we implement a flexible authentication framework for multiple platforms authentication using wearable devices that are highly customizable.","tags":null,"title":"Towards a Gratifying Interactive Modality for Smart Environments based on Ubiquitous Sensing (BSc. Thesis)","type":"publication"},{"authors":null,"categories":null,"content":"We propose three multi-modal methods for mapping text and images of news articles to the shared space in order to perform efficient cross-retrieval.\n","date":1607904e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607904e3,"objectID":"1e3df8cdf32aeed6550f08a85a1c2af8","permalink":"https://duynht.github.io/publication/image_text/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/publication/image_text/","section":"publication","summary":"Matching text and images based on their semantics has an important role in cross-media retrieval. Especially, in terms of news, text and images connection is highly ambiguous. In the context of MediaEval 2020 Challenge, we propose three multi-modal methods for mapping text and images of news articles to the shared space in order to perform efficient cross-retrieval. Our methods show systemic improvement and validate our hypotheses, while the best-performed method reaches a recall@100 score of 0.2064.","tags":null,"title":"HCMUS at MediaEval 2020: Image-Text Fusion for Automatic News-Images Re-Matching (MediaEval 2020)","type":"publication"},{"authors":null,"categories":null,"content":"We employ 3 methods, FaceNet, Siamese VGGFace, and a combination of FaceNet and VGG-Face models as feature extractors, to achieve the 9th standing for kinship verification and the 5th standing for kinship retrieval in the Recognizing Family in The Wild 2020 competition.\n","date":1605484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605484800,"objectID":"bd99de61187bd0062bafe7cf7ffeebd4","permalink":"https://duynht.github.io/publication/kinship/","publishdate":"2020-11-16T00:00:00Z","relpermalink":"/publication/kinship/","section":"publication","summary":"Kinship verification and kinship retrieval are emerging tasks in computer vision. Kinship verification aims at determining whether two facial images are from related people or not, while kinship retrieval is the task of retrieving possible related facial images to a person from a gallery of images. They introduce unique challenges because of the hidden relations and features that carry inherent characteristics between the facial images. We employ 3 methods, FaceNet, Siamese VGGFace, and a combination of FaceNet and VGG-Face models as feature extractors, to achieve the 9th standing for kinship verification and the 5th standing for kinship retrieval in the Recognizing Family in The Wild 2020 competition. We then further experimented using StyleGAN2 as another encoder, with no improvement in the result.","tags":null,"title":"Recognizing Families through Images with Pretrained Encoder (FG 2020)","type":"publication"},{"authors":null,"categories":null,"content":"This paper investigates a method of generalization through adversarial data augmentation.\n","date":1584489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584489600,"objectID":"b138d9ccfa59782fa92e5e6ec24430dc","permalink":"https://duynht.github.io/publication/wifi/","publishdate":"2020-03-18T00:00:00Z","relpermalink":"/publication/wifi/","section":"publication","summary":"Recent WiFi-based fall detection systems have drawn much attention due to their advantages over other sensory systems. Various implementations have achieved impressive progress in performance, thanks to machine learning and deep learning techniques. However, many of such high accuracy systems have low reliability as they fail to achieve robustness in unseen environments. To address that, this paper investigates a method of generalization through adversarial data augmentation. Our results show a slight improvement in deep learning systems in unseen domains, though the performance is not significant.","tags":null,"title":"Towards a Robust WiFi-based Fall Detection with Adversarial Data Augmentation (CISS 2020)","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://duynht.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]